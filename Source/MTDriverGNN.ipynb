{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvNo7S8mg1-P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import copy\n",
        "import json\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import to_undirected, coalesce\n",
        "\n",
        "# ----------------- BASIC CONFIG & UTILS ----------------- #\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    # General training\n",
        "    num_epochs: int = 300\n",
        "    patience: int = 30\n",
        "    kfold: int = 5\n",
        "    warmup: int = 10\n",
        "\n",
        "    # Pretrain (cross-disease)\n",
        "    pretrain_epochs: int = 300\n",
        "    pretrain_patience: int = 30\n",
        "    pretrain_lr: float = 1e-2\n",
        "\n",
        "    # Search space\n",
        "    depth_options: Tuple[int, ...] = (1, 2)\n",
        "    hidden_options: Tuple[Tuple[int, int], ...] = ((64, 64), (64, 128), (128, 128))\n",
        "    dropout_options: Tuple[float, ...] = (0.3, 0.4, 0.5)\n",
        "    lr_options: Tuple[float, ...] = (1e-2, 3e-3, 1e-3)\n",
        "    weight_decay_options: Tuple[float, ...] = (1e-4, 5e-4, 1e-3)\n",
        "\n",
        "    # IO\n",
        "    base_dir: str = \"/content/drive/MyDrive/....\"\n",
        "    target_disease: str = \"BRCA\"\n",
        "    results_dir: str = \"./results_mtdriver\"\n",
        "\n",
        "# ----------------- DATA & LABEL LOADING ----------------- #\n",
        "\n",
        "def load_graph_and_features(cfg: TrainConfig):\n",
        "    ppi_path = os.path.join(cfg.base_dir, \"PPI_CPDB.csv\")\n",
        "    ppi_df = pd.read_csv(ppi_path)\n",
        "\n",
        "    col1, col2 = ppi_df.columns[:2]\n",
        "    g1 = ppi_df[col1].astype(str).values\n",
        "    g2 = ppi_df[col2].astype(str).values\n",
        "\n",
        "    genes_edge = np.unique(np.concatenate([g1, g2]))\n",
        "\n",
        "    feat_path = os.path.join(cfg.base_dir, f\"features_for_{cfg.target_disease}.csv\")\n",
        "    feat_df = pd.read_csv(feat_path, index_col=0)\n",
        "    feat_df.index = feat_df.index.astype(str)\n",
        "    feat_genes = feat_df.index.values\n",
        "\n",
        "    all_genes = np.unique(np.concatenate([genes_edge, feat_genes]))\n",
        "    node_names = all_genes\n",
        "    num_nodes = len(node_names)\n",
        "    print(f\"[INFO] #nodes (CPDB union features) = {num_nodes}\")\n",
        "\n",
        "    node_to_idx: Dict[str, int] = {g: i for i, g in enumerate(node_names)}\n",
        "\n",
        "    src = np.array([node_to_idx[g] for g in g1], dtype=np.int64)\n",
        "    dst = np.array([node_to_idx[g] for g in g2], dtype=np.int64)\n",
        "    edge_index = torch.tensor(np.stack([src, dst], axis=0), dtype=torch.long)\n",
        "\n",
        "    edge_index = to_undirected(edge_index, num_nodes=num_nodes)\n",
        "    edge_index, _ = coalesce(edge_index, None, num_nodes, num_nodes)\n",
        "    print(f\"[INFO] #edges after undirected+coalesce = {edge_index.size(1)}\")\n",
        "\n",
        "    feature_dim = feat_df.shape[1]\n",
        "    x_mat = np.zeros((num_nodes, feature_dim), dtype=np.float32)\n",
        "    has_feat = np.zeros(num_nodes, dtype=bool)\n",
        "\n",
        "    genes_in_both = feat_df.index.intersection(pd.Index(node_names))\n",
        "    print(f\"[INFO] #genes with features ∩ graph = {len(genes_in_both)}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    feat_scaled = scaler.fit_transform(feat_df.loc[genes_in_both].values)\n",
        "    feat_scaled_df = pd.DataFrame(feat_scaled, index=genes_in_both, columns=feat_df.columns)\n",
        "\n",
        "    for g in genes_in_both:\n",
        "        idx = node_to_idx[g]\n",
        "        x_mat[idx] = feat_scaled_df.loc[g].values\n",
        "        has_feat[idx] = True\n",
        "\n",
        "    neighbors = {i: [] for i in range(num_nodes)}\n",
        "    for u, v in edge_index.t().tolist():\n",
        "        neighbors[u].append(v)\n",
        "        neighbors[v].append(u)\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        if not has_feat[i]:\n",
        "            nb = [x_mat[n] for n in neighbors[i] if has_feat[n]]\n",
        "            if nb:\n",
        "                x_mat[i] = np.mean(nb, axis=0)\n",
        "\n",
        "    x = torch.tensor(x_mat, dtype=torch.float32)\n",
        "    data = Data(x=x, edge_index=edge_index)\n",
        "    return data, node_names, node_to_idx\n",
        "\n",
        "def load_labels(cfg: TrainConfig, data: Data, node_to_idx: Dict[str, int]):\n",
        "    num_nodes = data.num_nodes\n",
        "    y1 = torch.full((num_nodes,), -1, dtype=torch.long)\n",
        "    y2 = torch.full((num_nodes,), -1, dtype=torch.long)\n",
        "\n",
        "    y1_path = os.path.join(cfg.base_dir, f\"{cfg.target_disease}_labels(0_1).csv\")\n",
        "    y1_df = pd.read_csv(y1_path)\n",
        "    y1_df[\"Gene\"] = y1_df[\"Gene\"].astype(str)\n",
        "    y1_map = dict(zip(y1_df['Gene'], y1_df['Labels']))\n",
        "    for g, lab in y1_map.items():\n",
        "        if g in node_to_idx:\n",
        "            y1[node_to_idx[g]] = int(lab)\n",
        "\n",
        "    y2_path = os.path.join(cfg.base_dir, \"dataset\", \"label_telomere.csv\")\n",
        "    y2_df = pd.read_csv(y2_path)\n",
        "    y2_df[\"Gene\"] = y2_df[\"Gene\"].astype(str)\n",
        "    y2_map = dict(zip(y2_df['Gene'], y2_df['Labels']))\n",
        "    for g, lab in y2_map.items():\n",
        "        if g in node_to_idx:\n",
        "            y2[node_to_idx[g]] = int(lab)\n",
        "\n",
        "    data = data.to(DEVICE)\n",
        "    data.y = y1.to(DEVICE)\n",
        "    data.y2 = y2.to(DEVICE)\n",
        "    labeled_idx = (data.y != -1).nonzero(as_tuple=True)[0]\n",
        "    print(f\"[INFO] #labeled (y1 != -1) = {len(labeled_idx)}\")\n",
        "    return data, labeled_idx\n",
        "\n",
        "# ----------------- PRETRAIN META ----------------- #\n",
        "\n",
        "def build_disease_dict(base_dir: str) -> Dict[str, Dict[str, str]]:\n",
        "    disease_list = [\"BRCA\", \"LUAD\", \"CESC\", \"BLCA\", \"LIHC\", \"THCA\",\n",
        "                    \"ESCA\", \"PRAD\", \"STAD\", \"COAD\", \"UCEC\", \"LUSC\"]\n",
        "    diseases = {\n",
        "        d: {\n",
        "            \"Y1\": os.path.join(base_dir, f\"{d}_labels(0_1).csv\"),\n",
        "        }\n",
        "        for d in disease_list\n",
        "    }\n",
        "    return diseases\n",
        "\n",
        "# ----------------- MODEL DEFINITIONS ----------------- #\n",
        "\n",
        "class ResidualGCNEncoder(nn.Module):\n",
        "    def __init__(self, in_dim: int, hidden_dims: List[int],\n",
        "                 dropout: float = 0.5, use_layernorm: bool = False):\n",
        "        super().__init__()\n",
        "        assert len(hidden_dims) >= 1\n",
        "        self.convs = nn.ModuleList()\n",
        "        last = in_dim\n",
        "        for h in hidden_dims:\n",
        "            self.convs.append(GCNConv(last, h))\n",
        "            last = h\n",
        "        self.res_proj = nn.Linear(in_dim, hidden_dims[-1], bias=False) if in_dim != hidden_dims[-1] else nn.Identity()\n",
        "        self.ln = nn.LayerNorm(hidden_dims[-1]) if use_layernorm else nn.Identity()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        for conv in self.convs:\n",
        "            nn.init.xavier_uniform_(conv.lin.weight)\n",
        "        if isinstance(self.res_proj, nn.Linear):\n",
        "            nn.init.xavier_uniform_(self.res_proj.weight)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        h = x\n",
        "        for conv in self.convs:\n",
        "            h = conv(h, edge_index)\n",
        "            h = F.relu(h)\n",
        "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        h = h + (self.res_proj(x) if isinstance(self.res_proj, nn.Linear) else x)\n",
        "        h = self.ln(h)\n",
        "        return h\n",
        "\n",
        "class GCN_Residual_TwoHeads(nn.Module):\n",
        "    def __init__(self, in_dim: int, hidden_dims: List[int],\n",
        "                 dropout: float = 0.5, use_layernorm: bool = False,\n",
        "                 head_hidden: int = None):\n",
        "        super().__init__()\n",
        "        self.encoder = ResidualGCNEncoder(in_dim, hidden_dims,\n",
        "                                          dropout=dropout,\n",
        "                                          use_layernorm=use_layernorm)\n",
        "        hd = hidden_dims[-1]\n",
        "        if head_hidden is None:\n",
        "            head_hidden = hd\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(hd, head_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.out_y1 = nn.Linear(head_hidden, 1)\n",
        "        self.out_y2 = nn.Linear(head_hidden, 1)\n",
        "\n",
        "        for m in self.shared:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")\n",
        "        nn.init.kaiming_uniform_(self.out_y1.weight, nonlinearity=\"sigmoid\")\n",
        "        nn.init.kaiming_uniform_(self.out_y2.weight, nonlinearity=\"sigmoid\")\n",
        "\n",
        "    def forward(self, x, edge_index, return_h: bool = False):\n",
        "        h = self.encoder(x, edge_index)\n",
        "        h = self.shared(h)\n",
        "        logit1 = self.out_y1(h).squeeze(-1)\n",
        "        logit2 = self.out_y2(h).squeeze(-1)\n",
        "        if return_h:\n",
        "            return logit1, logit2, h\n",
        "        return logit1, logit2\n",
        "\n",
        "class LearnableAlpha(nn.Module):\n",
        "    def __init__(self, init_alpha: float = 0.7):\n",
        "        super().__init__()\n",
        "        init_logit = torch.logit(torch.tensor(float(init_alpha)))\n",
        "        self.logit_alpha = nn.Parameter(init_logit)\n",
        "    def forward(self):\n",
        "        return torch.sigmoid(self.logit_alpha)\n",
        "\n",
        "# ----------------- LOSS / METRIC UTILS ----------------- #\n",
        "\n",
        "@torch.no_grad()\n",
        "def auprc_on_mask(logits: torch.Tensor, y_long: torch.Tensor, mask: torch.Tensor) -> float:\n",
        "    if mask.sum().item() == 0:\n",
        "        return 0.0\n",
        "    labels = y_long[mask].detach().cpu().numpy()\n",
        "    if (labels == 1).sum() == 0 or (labels == 0).sum() == 0:\n",
        "        return 0.0\n",
        "    probs = torch.sigmoid(logits[mask]).detach().cpu().numpy()\n",
        "    return float(average_precision_score(labels, probs))\n",
        "\n",
        "def bce_pos_weight_from_mask(y_long: torch.Tensor, mask: torch.Tensor, device) -> torch.Tensor:\n",
        "    yy = y_long[mask]\n",
        "    if yy.numel() == 0:\n",
        "        return None\n",
        "    pos = (yy == 1).sum().item()\n",
        "    neg = (yy == 0).sum().item()\n",
        "    if pos == 0:\n",
        "        return None\n",
        "    return torch.tensor([neg / float(pos)], dtype=torch.float, device=device)\n",
        "\n",
        "# ----------------- CROSS-DISEASE PRETRAIN ----------------- #\n",
        "\n",
        "def build_cross_disease_pretrain_labels(\n",
        "    node_to_idx: Dict[str, int],\n",
        "    target_name: str,\n",
        "    data_y: torch.Tensor,\n",
        "    base_dir: str\n",
        "):\n",
        "    diseases = build_disease_dict(base_dir)\n",
        "    num_nodes = len(node_to_idx)\n",
        "    y_pre = torch.full((num_nodes,), -1, dtype=torch.long)\n",
        "    has_any_label = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "    target_labeled_mask = (data_y.detach().cpu() != -1)\n",
        "\n",
        "    for d, meta in diseases.items():\n",
        "        if d == target_name:\n",
        "            continue\n",
        "        df = pd.read_csv(meta[\"Y1\"])\n",
        "        df[\"Gene\"] = df[\"Gene\"].astype(str)\n",
        "        for g, lab in zip(df[\"Gene\"], df[\"Labels\"]):\n",
        "            if g in node_to_idx:\n",
        "                idx = node_to_idx[g]\n",
        "                has_any_label[idx] = True\n",
        "                if int(lab) == 1:\n",
        "                    y_pre[idx] = 1\n",
        "\n",
        "    unlabeled_any = (~has_any_label)\n",
        "    y_pre[unlabeled_any] = 0\n",
        "\n",
        "    pretrain_mask = ((y_pre == 1) | (y_pre == 0)) & (~target_labeled_mask)\n",
        "\n",
        "    pos_ct = int(((y_pre == 1) & pretrain_mask).sum().item())\n",
        "    neg_ct = int(((y_pre == 0) & pretrain_mask).sum().item())\n",
        "    excl   = int((target_labeled_mask & ((y_pre == 1) | (y_pre == 0))).sum().item())\n",
        "    print(f\"[Pretrain-XDisease] Pos: {pos_ct} | Neg: {neg_ct} | Excluded target-labeled: {excl}\")\n",
        "    return y_pre.to(DEVICE), pretrain_mask.to(DEVICE)\n",
        "\n",
        "def pretrain_on_cross_disease(\n",
        "    data: Data,\n",
        "    y_pre: torch.Tensor,\n",
        "    pretrain_mask: torch.Tensor,\n",
        "    hidden_dims: List[int],\n",
        "    cfg: TrainConfig,\n",
        "    dropout: float,\n",
        "    weight_decay: float,\n",
        "):\n",
        "    model = GCN_Residual_TwoHeads(\n",
        "        in_dim=data.num_features,\n",
        "        hidden_dims=hidden_dims,\n",
        "        dropout=dropout,\n",
        "        use_layernorm=False,\n",
        "        head_hidden=None,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=cfg.pretrain_lr,\n",
        "        weight_decay=weight_decay,\n",
        "    )\n",
        "\n",
        "    yy = y_pre[pretrain_mask]\n",
        "    pos = int((yy == 1).sum().item())\n",
        "    neg = int((yy == 0).sum().item())\n",
        "    if pos == 0:\n",
        "        print(\"[Pretrain-XDisease] No positive samples; skipping pretraining.\")\n",
        "        return None\n",
        "\n",
        "    pos_weight = torch.tensor([neg / float(pos)], dtype=torch.float, device=DEVICE)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "    best_state = copy.deepcopy(model.state_dict())\n",
        "    best_loss = float(\"inf\")\n",
        "    wait = 0\n",
        "\n",
        "    for ep in range(1, cfg.pretrain_epochs + 1):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        logit1, _ = model(data.x, data.edge_index)\n",
        "        loss = criterion(logit1[pretrain_mask], y_pre[pretrain_mask].float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cur_loss = float(loss.item())\n",
        "        if cur_loss < best_loss - 1e-6:\n",
        "            best_loss = cur_loss\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= cfg.pretrain_patience:\n",
        "                print(f\"[Pretrain-XDisease] Early stop @ epoch {ep}, best_loss={best_loss:.4f}\")\n",
        "                break\n",
        "\n",
        "    pre_state = {\n",
        "        k: v\n",
        "        for k, v in best_state.items()\n",
        "        if k.startswith(\"encoder.\") or k.startswith(\"shared.\") or k.startswith(\"out_y1.\")\n",
        "    }\n",
        "    return pre_state\n",
        "\n",
        "# ----------------- TRAIN / EVAL (MULTITASK) ----------------- #\n",
        "\n",
        "def train_one_epoch_dual(\n",
        "    model: nn.Module,\n",
        "    data: Data,\n",
        "    train_mask: torch.Tensor,\n",
        "    y2_mask_train: torch.Tensor,\n",
        "    opt: torch.optim.Optimizer,\n",
        "    crit_y1,\n",
        "    crit_y2,\n",
        "    alpha_module: LearnableAlpha,\n",
        "    epoch: int,\n",
        "    warmup: int = 10,\n",
        "):\n",
        "    model.train()\n",
        "    opt.zero_grad()\n",
        "    logit1, logit2 = model(data.x, data.edge_index)\n",
        "\n",
        "    loss1 = (\n",
        "        crit_y1(logit1[train_mask], data.y[train_mask].float())\n",
        "        if (crit_y1 is not None and train_mask.sum().item() > 0)\n",
        "        else torch.tensor(0.0, device=logit1.device)\n",
        "    )\n",
        "    loss2 = (\n",
        "        crit_y2(logit2[y2_mask_train], data.y2[y2_mask_train].float())\n",
        "        if (crit_y2 is not None and y2_mask_train.sum().item() > 0)\n",
        "        else torch.tensor(0.0, device=logit2.device)\n",
        "    )\n",
        "\n",
        "    if epoch < warmup:\n",
        "        total = loss1\n",
        "        alpha_val = 1.0\n",
        "    else:\n",
        "        alpha = alpha_module()\n",
        "        total = alpha * loss1 + (1.0 - alpha) * loss2\n",
        "        alpha_val = float(alpha.item())\n",
        "\n",
        "    total.backward()\n",
        "    opt.step()\n",
        "    return float(loss1.item()), float(loss2.item()), float(total.item()), alpha_val\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_y1(model: nn.Module, data: Data, mask: torch.Tensor, criterion_y1):\n",
        "    model.eval()\n",
        "    logit1, _ = model(data.x, data.edge_index)\n",
        "    loss = (\n",
        "        criterion_y1(logit1[mask], data.y[mask].float())\n",
        "        if (criterion_y1 is not None and mask.sum().item() > 0)\n",
        "        else 0.0\n",
        "    )\n",
        "    auprc = auprc_on_mask(logit1, data.y, mask)\n",
        "    return float(loss), float(auprc)\n",
        "\n",
        "# ----------------- GRID SEARCH ----------------- #\n",
        "\n",
        "def grid_search_for_outer_fold(\n",
        "    data: Data,\n",
        "    outer_trainval_idx: torch.Tensor,\n",
        "    outer_test_idx: torch.Tensor,\n",
        "    y_pre: torch.Tensor,\n",
        "    pretrain_mask: torch.Tensor,\n",
        "    cfg: TrainConfig,\n",
        "    base_seed: int,\n",
        "):\n",
        "\n",
        "    best_hp = None\n",
        "    best_val_auc = -1.0\n",
        "\n",
        "    y_tv = data.y[outer_trainval_idx].detach().cpu().numpy()\n",
        "\n",
        "    for depth in cfg.depth_options:\n",
        "        for hd_pair in cfg.hidden_options:\n",
        "            hidden_dims = [hd_pair[0]] if depth == 1 else list(hd_pair)\n",
        "            for dr in cfg.dropout_options:\n",
        "                for lr in cfg.lr_options:\n",
        "                    for wd in cfg.weight_decay_options:\n",
        "                        print(\"\\n[GRID-FOLD] depth={}, hidden={}, drop={}, lr={}, wd={}\".format(\n",
        "                            depth, hidden_dims, dr, lr, wd))\n",
        "\n",
        "                        set_seed(base_seed + 1000)\n",
        "\n",
        "                        pretrained_state = pretrain_on_cross_disease(\n",
        "                            data=data,\n",
        "                            y_pre=y_pre,\n",
        "                            pretrain_mask=pretrain_mask,\n",
        "                            hidden_dims=hidden_dims,\n",
        "                            cfg=cfg,\n",
        "                            dropout=dr,\n",
        "                            weight_decay=wd,\n",
        "                        )\n",
        "\n",
        "\n",
        "                        sss = StratifiedShuffleSplit(\n",
        "                            n_splits=1,\n",
        "                            test_size=0.2,\n",
        "                            random_state=base_seed + 1234,\n",
        "                        )\n",
        "                        tr_sub, va_sub = next(sss.split(outer_trainval_idx.cpu(), y_tv))\n",
        "                        inner_train_nodes = outer_trainval_idx[tr_sub].to(DEVICE)\n",
        "                        inner_val_nodes   = outer_trainval_idx[va_sub].to(DEVICE)\n",
        "\n",
        "                        N = data.num_nodes\n",
        "                        train_mask = torch.zeros(N, dtype=torch.bool, device=DEVICE)\n",
        "                        val_mask   = torch.zeros(N, dtype=torch.bool, device=DEVICE)\n",
        "                        test_mask  = torch.zeros(N, dtype=torch.bool, device=DEVICE)\n",
        "                        train_mask[inner_train_nodes] = True\n",
        "                        val_mask[inner_val_nodes]     = True\n",
        "                        test_mask[outer_test_idx.to(DEVICE)] = True\n",
        "\n",
        "                        y2_mask_train = (data.y2 != -1) & (~test_mask)\n",
        "                        pw_y1 = bce_pos_weight_from_mask(data.y,  train_mask, DEVICE)\n",
        "                        pw_y2 = bce_pos_weight_from_mask(data.y2, y2_mask_train, DEVICE)\n",
        "                        crit_y1 = nn.BCEWithLogitsLoss(pos_weight=pw_y1) if pw_y1 is not None else None\n",
        "                        crit_y2 = nn.BCEWithLogitsLoss(pos_weight=pw_y2) if pw_y2 is not None else None\n",
        "\n",
        "                        model = GCN_Residual_TwoHeads(\n",
        "                            in_dim=data.num_features,\n",
        "                            hidden_dims=hidden_dims,\n",
        "                            dropout=dr,\n",
        "                            use_layernorm=False,\n",
        "                            head_hidden=None,\n",
        "                        ).to(DEVICE)\n",
        "\n",
        "                        if pretrained_state is not None:\n",
        "                            model.load_state_dict(pretrained_state, strict=False)\n",
        "\n",
        "                        alpha_module = LearnableAlpha(init_alpha=0.7).to(DEVICE)\n",
        "                        optimizer = torch.optim.Adam(\n",
        "                            list(model.parameters()) + list(alpha_module.parameters()),\n",
        "                            lr=lr,\n",
        "                            weight_decay=wd,\n",
        "                        )\n",
        "\n",
        "                        best_val_auc_hp = -1.0\n",
        "                        wait = 0\n",
        "                        for ep in range(1, cfg.num_epochs + 1):\n",
        "                            l1, l2, ltot, a = train_one_epoch_dual(\n",
        "                                model, data,\n",
        "                                train_mask, y2_mask_train,\n",
        "                                optimizer, crit_y1, crit_y2,\n",
        "                                alpha_module, ep, warmup=cfg.warmup\n",
        "                            )\n",
        "                            _, v_auc = evaluate_y1(model, data, val_mask, crit_y1)\n",
        "\n",
        "                            if v_auc > best_val_auc_hp:\n",
        "                                best_val_auc_hp = v_auc\n",
        "                                wait = 0\n",
        "                            else:\n",
        "                                wait += 1\n",
        "                                if wait >= cfg.patience:\n",
        "                                    break\n",
        "\n",
        "                        print(f\"[GRID-FOLD] Val AUPRC = {best_val_auc_hp:.4f}\")\n",
        "\n",
        "                        if best_val_auc_hp > best_val_auc:\n",
        "                            best_val_auc = best_val_auc_hp\n",
        "                            best_hp = {\n",
        "                                \"depth\": depth,\n",
        "                                \"hidden_dims\": hidden_dims,\n",
        "                                \"dropout\": dr,\n",
        "                                \"lr\": lr,\n",
        "                                \"weight_decay\": wd,\n",
        "                            }\n",
        "\n",
        "    print(f\"[GRID-FOLD] Best HP for this outer fold: {best_hp}, Val AUPRC={best_val_auc:.4f}\")\n",
        "    return best_hp\n",
        "\n",
        "# ----------------- FINAL TRAIN + TEST FOR 1 OUTER FOLD ----------------- #\n",
        "\n",
        "def train_and_test_one_outer_fold(\n",
        "    data: Data,\n",
        "    outer_trainval_idx: torch.Tensor,\n",
        "    outer_test_idx: torch.Tensor,\n",
        "    y_pre: torch.Tensor,\n",
        "    pretrain_mask: torch.Tensor,\n",
        "    cfg: TrainConfig,\n",
        "    hp: Dict[str, Any],\n",
        "    base_seed: int,\n",
        "):\n",
        "\n",
        "    depth = hp[\"depth\"]\n",
        "    hidden_dims = hp[\"hidden_dims\"]\n",
        "    dr = hp[\"dropout\"]\n",
        "    lr = hp[\"lr\"]\n",
        "    wd = hp[\"weight_decay\"]\n",
        "\n",
        "    set_seed(base_seed + 2000)\n",
        "\n",
        "\n",
        "    pretrained_state = pretrain_on_cross_disease(\n",
        "        data=data,\n",
        "        y_pre=y_pre,\n",
        "        pretrain_mask=pretrain_mask,\n",
        "        hidden_dims=hidden_dims,\n",
        "        cfg=cfg,\n",
        "        dropout=dr,\n",
        "        weight_decay=wd,\n",
        "    )\n",
        "\n",
        "    y_tv = data.y[outer_trainval_idx].detach().cpu().numpy()\n",
        "    sss = StratifiedShuffleSplit(\n",
        "        n_splits=1,\n",
        "        test_size=0.2,\n",
        "        random_state=base_seed + 2345,\n",
        "    )\n",
        "    tr_sub, va_sub = next(sss.split(outer_trainval_idx.cpu(), y_tv))\n",
        "    inner_train_nodes = outer_trainval_idx[tr_sub].to(DEVICE)\n",
        "    inner_val_nodes   = outer_trainval_idx[va_sub].to(DEVICE)\n",
        "\n",
        "    N = data.num_nodes\n",
        "    train_mask = torch.zeros(N, dtype=torch.bool, device=DEVICE)\n",
        "    val_mask   = torch.zeros(N, dtype=torch.bool, device=DEVICE)\n",
        "    test_mask  = torch.zeros(N, dtype=torch.bool, device=DEVICE)\n",
        "    train_mask[inner_train_nodes] = True\n",
        "    val_mask[inner_val_nodes]     = True\n",
        "    test_mask[outer_test_idx.to(DEVICE)] = True\n",
        "\n",
        "    y2_mask_train = (data.y2 != -1) & (~test_mask)\n",
        "    pw_y1 = bce_pos_weight_from_mask(data.y,  train_mask, DEVICE)\n",
        "    pw_y2 = bce_pos_weight_from_mask(data.y2, y2_mask_train, DEVICE)\n",
        "    crit_y1 = nn.BCEWithLogitsLoss(pos_weight=pw_y1) if pw_y1 is not None else None\n",
        "    crit_y2 = nn.BCEWithLogitsLoss(pos_weight=pw_y2) if pw_y2 is not None else None\n",
        "\n",
        "    model = GCN_Residual_TwoHeads(\n",
        "        in_dim=data.num_features,\n",
        "        hidden_dims=hidden_dims,\n",
        "        dropout=dr,\n",
        "        use_layernorm=False,\n",
        "        head_hidden=None,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    if pretrained_state is not None:\n",
        "        model.load_state_dict(pretrained_state, strict=False)\n",
        "\n",
        "    alpha_module = LearnableAlpha(init_alpha=0.7).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(\n",
        "        list(model.parameters()) + list(alpha_module.parameters()),\n",
        "        lr=lr,\n",
        "        weight_decay=wd,\n",
        "    )\n",
        "\n",
        "    best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
        "    best_val_auc = -1.0\n",
        "    wait = 0\n",
        "\n",
        "    for ep in range(1, cfg.num_epochs + 1):\n",
        "        l1, l2, ltot, a = train_one_epoch_dual(\n",
        "            model, data,\n",
        "            train_mask, y2_mask_train,\n",
        "            optimizer, crit_y1, crit_y2,\n",
        "            alpha_module, ep, warmup=cfg.warmup\n",
        "        )\n",
        "        _, v_auc = evaluate_y1(model, data, val_mask, crit_y1)\n",
        "\n",
        "        if v_auc > best_val_auc:\n",
        "            best_val_auc = v_auc\n",
        "            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= cfg.patience:\n",
        "                break\n",
        "\n",
        "\n",
        "    model.load_state_dict(best_state, strict=True)\n",
        "    logit1, _ = model(data.x, data.edge_index)\n",
        "    test_auprc = auprc_on_mask(logit1, data.y, test_mask)\n",
        "    print(f\"[OUTER-FOLD] Test AUPRC = {test_auprc:.4f}\")\n",
        "    return test_auprc\n",
        "\n",
        "\n",
        "def nested_cv_one_run(\n",
        "    data: Data,\n",
        "    labeled_idx: torch.Tensor,\n",
        "    y_pre: torch.Tensor,\n",
        "    pretrain_mask: torch.Tensor,\n",
        "    cfg: TrainConfig,\n",
        "    base_seed: int,\n",
        "):\n",
        "    skf = StratifiedKFold(\n",
        "        n_splits=cfg.kfold,\n",
        "        shuffle=True,\n",
        "        random_state=base_seed,\n",
        "    )\n",
        "    y_np = data.y[labeled_idx].detach().cpu().numpy()\n",
        "    fold_test_scores = []\n",
        "\n",
        "    for fold, (tr_idx, te_idx) in enumerate(skf.split(labeled_idx.cpu(), y_np), start=1):\n",
        "        print(f\"\\n========== RUN seed={base_seed} | OUTER FOLD {fold}/{cfg.kfold} ==========\")\n",
        "        outer_trainval_idx = labeled_idx[tr_idx]\n",
        "        outer_test_idx     = labeled_idx[te_idx]\n",
        "\n",
        "\n",
        "        best_hp = grid_search_for_outer_fold(\n",
        "            data, outer_trainval_idx, outer_test_idx,\n",
        "            y_pre, pretrain_mask,\n",
        "            cfg=cfg,\n",
        "            base_seed=base_seed + fold * 10,\n",
        "        )\n",
        "\n",
        "\n",
        "        test_auc = train_and_test_one_outer_fold(\n",
        "            data, outer_trainval_idx, outer_test_idx,\n",
        "            y_pre, pretrain_mask,\n",
        "            cfg=cfg,\n",
        "            hp=best_hp,\n",
        "            base_seed=base_seed + fold * 20,\n",
        "        )\n",
        "        fold_test_scores.append(test_auc)\n",
        "\n",
        "    mean_auc = float(np.mean(fold_test_scores))\n",
        "    std_auc  = float(np.std(fold_test_scores))\n",
        "    print(f\"\\n[RUN seed={base_seed}] Mean Test AUPRC over {cfg.kfold} folds: {mean_auc:.4f} ± {std_auc:.4f}\")\n",
        "    return fold_test_scores, mean_auc, std_auc\n",
        "\n",
        "# ----------------- MAIN: 10 RUNS (SEEDS 42..51) ----------------- #\n",
        "\n",
        "def main():\n",
        "    cfg = TrainConfig()\n",
        "    os.makedirs(cfg.results_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    data, node_names, node_to_idx = load_graph_and_features(cfg)\n",
        "    data, labeled_idx = load_labels(cfg, data, node_to_idx)\n",
        "\n",
        "\n",
        "    y_pre, pretrain_mask = build_cross_disease_pretrain_labels(\n",
        "        node_to_idx=node_to_idx,\n",
        "        target_name=cfg.target_disease,\n",
        "        data_y=data.y,\n",
        "        base_dir=cfg.base_dir,\n",
        "    )\n",
        "\n",
        "    seeds = list(range(42, 43))  # 10 runs\n",
        "    all_run_results = []\n",
        "\n",
        "    for i, seed in enumerate(seeds, start=1):\n",
        "        print(\"\\n=========================================\")\n",
        "        print(f\"======== NESTED CV RUN {i}/10 — seed = {seed} =========\")\n",
        "        print(\"=========================================\")\n",
        "\n",
        "        set_seed(seed)\n",
        "        fold_scores, mean_auc, std_auc = nested_cv_one_run(\n",
        "            data, labeled_idx,\n",
        "            y_pre, pretrain_mask,\n",
        "            cfg=cfg,\n",
        "            base_seed=seed,\n",
        "        )\n",
        "\n",
        "        all_run_results.append({\n",
        "            \"seed\": seed,\n",
        "            \"fold_test_auprc\": fold_scores,\n",
        "            \"mean_test_auprc\": mean_auc,\n",
        "            \"std_test_auprc\": std_auc,\n",
        "        })\n",
        "\n",
        "\n",
        "    out_path = os.path.join(cfg.results_dir, f\"nested_cv_runs_{cfg.target_disease}.json\")\n",
        "    with open(out_path, \"w\") as f:\n",
        "        json.dump(all_run_results, f, indent=2)\n",
        "    print(f\"\\n[INFO] Nested CV 10-run results saved to: {out_path}\")\n",
        "\n",
        "    all_means = [r[\"mean_test_auprc\"] for r in all_run_results]\n",
        "    print(\"\\n========== 10-RUN SUMMARY (Nested CV) ==========\")\n",
        "    print(f\"Mean of run-wise mean Test AUPRC: {np.mean(all_means):.4f} ± {np.std(all_means):.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}